{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189e3d68-ab7a-474d-bc8c-74c4aca3639b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q transformers datasets evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f577c76-4284-48b7-9fef-78ad4c5872be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b5bb87-95a2-4908-b95f-bf6a122f8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d196b05d-7fcc-4d16-9076-56931cebb29c",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a89234-e065-4718-b29a-67a4e09a3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to tokenize and re-align tokens with labels due to the model tokenizer special tokens added\n",
    "def tokenize_align_labels(data):\n",
    "    # tokenize inputs\n",
    "    tokenized_inputs = tokenizer(data[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "\n",
    "    # For storing newly aligned label sequences for each tokenized input\n",
    "    labels = []\n",
    "\n",
    "    # iterate on each set of labels\n",
    "    for i, label in enumerate(data[\"ner_tags\"]):\n",
    "        # map tokens to their words respectively\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "\n",
    "        # keep track of word boundaries\n",
    "        previous_word_index = None\n",
    "\n",
    "        # empty list to store new labels for the current sequence\n",
    "        current_label_ids = []\n",
    "\n",
    "        for word_index in word_ids:\n",
    "            # if special token\n",
    "            if word_index is None:\n",
    "                current_label_ids.append(-100)\n",
    "            # new word\n",
    "            elif word_index != previous_word_index:\n",
    "                current_label_ids.append(label[word_index])\n",
    "            # sub token for the same word\n",
    "            else:\n",
    "                current_label_ids.append(-100)\n",
    "            previous_word_index = word_index\n",
    "        labels.append(current_label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e014a35-827c-4360-a3ee-dca6eda9cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    # (batch size, sequence length, labels percentage prediction)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    #print(predictions)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fad817-67c9-43e4-98e4-f360dd6ebe55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ded174-21f2-42cf-9eae-010a3af035ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfeac895-04c7-43e4-8991-e078864b9b3f",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9cd6f6-5035-4500-bebc-a90404098102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_dataset = load_dataset(\"conll2003\")\n",
    "con_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47118d3-05e5-43c6-8262-57962df000ce",
   "metadata": {},
   "source": [
    "### Exploring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6e2386e-6483-495f-b4f6-a0f78db49c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select ner_tags feature for analysis\n",
    "ner_features = con_dataset['train']['ner_tags']\n",
    "ner_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "447f4d80-5961-4e03-b56d-fa59ba9c7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for getting tags for analysis while negkecting the tags with I-\n",
    "tags = {'O': 0, 'B-PER': 1, 'xx': 2, 'B-ORG': 3, 'yy': 4, 'B-LOC': 5, 'zz': 6, 'B-MISC': 7, 'vv': 8}\n",
    "tags = dict(zip(tags.values(), tags.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1735b34-48ea-412e-8d7f-e49a1af55006",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_feature_names = [[tags[index] for index in feature] for feature in ner_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b9c59f8-c1d6-4b64-9388-77a6201981bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "038450ab-193a-4c72-acaa-4ae4b5b2220b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'O': 169578,\n",
       "         'B-LOC': 7140,\n",
       "         'B-PER': 6600,\n",
       "         'B-ORG': 6321,\n",
       "         'xx': 4528,\n",
       "         'yy': 3704,\n",
       "         'B-MISC': 3438,\n",
       "         'zz': 1157,\n",
       "         'vv': 1155})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten feature names for count\n",
    "dataset_names = itertools.chain.from_iterable(ner_feature_names)\n",
    "\n",
    "# get count of names\n",
    "collections.Counter(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc87f80b-a0e0-4406-a256-03b4a8bacd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum length of text 113\n",
      "minimum length of text 1\n"
     ]
    }
   ],
   "source": [
    "# Check for text lengths\n",
    "text_length = []\n",
    "text_feature = con_dataset['train']['tokens']\n",
    "[text_length.append(len(text_tokens)) for text_tokens in text_feature]\n",
    "print('maximum length of text', max(text_length))\n",
    "print('minimum length of text', min(text_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c4700-071a-4078-83e9-29de2db52b86",
   "metadata": {},
   "source": [
    "### We have the following counts in dataset\n",
    "#### location        : 7140\n",
    "#### person          : 6600\n",
    "#### organization    : 6321\n",
    "#### miscellaneous   : 3438\n",
    "#### maximum text len: 113\n",
    "#### minimum text len: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada61ca8-93c8-4b45-bee5-d2802988335c",
   "metadata": {},
   "source": [
    "## Model selection and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67514cb9-ab51-4e7c-bdb2-317f66309b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert/distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52171bfe-cd94-4518-9dff-5d69370bce16",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df82b69e-de9d-4d2b-b26a-6946cf72b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'eu',\n",
       " 'rejects',\n",
       " 'german',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'british',\n",
       " 'lamb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since input is tokenzied we set split into words to true\n",
    "tokenized_input = tokenizer(con_dataset[\"train\"][\"tokens\"][0], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb041a8b-2078-4f1d-89de-34105f7e84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and re-align tokens with labels due to the model tokenizer special tokens added\n",
    "con_dataset = con_dataset.map(tokenize_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08e76fe7-b4fd-4b5d-8516-782fc5f8dcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_dataset[\"train\"][\"ner_tags\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d2be29-72a6-4b95-bfe9-039ece9999b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_dataset[\"train\"][\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ef046bc-e18a-44cc-844a-f06fba0c221c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_dataset[\"train\"][\"labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22ac04aa-7987-4b91-8892-4f8083aae629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data collator for padding data\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8035e074-7a1b-4122-a9f4-60d89348e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "id2label = dict(zip(label2id.values(), label2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171da86c-7c75-492b-a189-4d40c028a7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05fc0cea-db6e-4607-bada-4664b05240f0",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c50f000-f467-48d6-9a96-befd78adabf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label2id),\n",
    "                                                        id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98e85b49-d943-4d01-a540-e056dfcca093",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "warm_up = int(EPOCHS * len(con_dataset[\"train\"]) * 0.1)\n",
    "logging_steps = math.ceil(len(con_dataset[\"train\"])/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e273db5e-6e73-472a-9864-80f8ba265434",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./distil_bert_ner\",\n",
    "                                  num_train_epochs=EPOCHS,\n",
    "                                  warmup_steps=warm_up,\n",
    "                                  per_device_train_batch_size=BATCH_SIZE,\n",
    "                                  per_device_eval_batch_size=BATCH_SIZE,\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  weight_decay = 0.01,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  metric_for_best_model=\"eval_loss\",\n",
    "                                  save_total_limit = 1,\n",
    "                                  load_best_model_at_end = True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de5136cc-de2c-4ef0-a8e6-6587a3958a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=con_dataset[\"train\"],\n",
    "    eval_dataset=con_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b7bdf20-63ab-4aea-b54c-2153a0c11a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8780' max='8780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8780/8780 08:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.233000</td>\n",
       "      <td>0.417926</td>\n",
       "      <td>0.445253</td>\n",
       "      <td>0.463312</td>\n",
       "      <td>0.454103</td>\n",
       "      <td>0.912172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.771140</td>\n",
       "      <td>0.821104</td>\n",
       "      <td>0.795338</td>\n",
       "      <td>0.967077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.074919</td>\n",
       "      <td>0.885289</td>\n",
       "      <td>0.901380</td>\n",
       "      <td>0.893262</td>\n",
       "      <td>0.980141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>0.060287</td>\n",
       "      <td>0.902548</td>\n",
       "      <td>0.918041</td>\n",
       "      <td>0.910229</td>\n",
       "      <td>0.983198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.055209</td>\n",
       "      <td>0.917137</td>\n",
       "      <td>0.929485</td>\n",
       "      <td>0.923270</td>\n",
       "      <td>0.984853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>0.050989</td>\n",
       "      <td>0.921223</td>\n",
       "      <td>0.932851</td>\n",
       "      <td>0.927001</td>\n",
       "      <td>0.986040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.053557</td>\n",
       "      <td>0.924466</td>\n",
       "      <td>0.939246</td>\n",
       "      <td>0.931797</td>\n",
       "      <td>0.986488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.054168</td>\n",
       "      <td>0.930023</td>\n",
       "      <td>0.939414</td>\n",
       "      <td>0.934695</td>\n",
       "      <td>0.986916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.059191</td>\n",
       "      <td>0.930569</td>\n",
       "      <td>0.940592</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.987442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.064534</td>\n",
       "      <td>0.928892</td>\n",
       "      <td>0.940929</td>\n",
       "      <td>0.934872</td>\n",
       "      <td>0.987053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8780, training_loss=0.18105949432268773, metrics={'train_runtime': 501.8521, 'train_samples_per_second': 279.784, 'train_steps_per_second': 17.495, 'total_flos': 1703092151653326.0, 'train_loss': 0.18105949432268773, 'epoch': 10.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528aaee4-0b02-4128-a4d4-425a1cec3fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1df64d12-02b0-4b24-abd5-e8c184516079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rm -r distil_bert_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66aa3621-d8f6-4d1a-a571-606d8039da34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.11326012760400772,\n",
       " 'eval_precision': 0.8776753088567949,\n",
       " 'eval_recall': 0.8930594900849859,\n",
       " 'eval_f1': 0.8853005704256254,\n",
       " 'eval_accuracy': 0.9773446753526435,\n",
       " 'eval_runtime': 2.6168,\n",
       " 'eval_samples_per_second': 1319.528,\n",
       " 'eval_steps_per_second': 82.542,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(con_dataset[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
